{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZiNvBegZEwFq"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import numpy as np #for mathematical calculations if any needed \n",
    "import pandas as pd #for playing with dataframe\n",
    "import tensorflow as tf \n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer #For tokenizing the words\n",
    "from keras.preprocessing.sequence import pad_sequences #for padding the words of same length\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv, os\n",
    "import re #for manipulating with regex\n",
    "import nltk #importing stopwords to be removed from the dataset\n",
    "from gensim.models import Word2Vec #for creating word embeddings using Word2Vec model(CBAG or Skipgram)/ Loading pretrained Word2Vec or Glove Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4WyLnQ0Fnhd"
   },
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "train_df = pd.read_csv(\"/content/drive/My Drive/Movie_Reviews/RNN/train_cleaned.csv\")\n",
    "test_df = pd.read_csv(\"/content/drive/My Drive/Movie_Reviews/RNN/test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_YAW-K4I8R2"
   },
   "outputs": [],
   "source": [
    "#input declearations \n",
    "embedding_size = 50\n",
    "sequence_length = 100\n",
    "corpus_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Rgly5eyWJLk-",
    "outputId": "44c4d4a1-75e8-4282-f941-4092bc0a4bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#setting up stopwords from the nltk library\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWaxTaoVJhzr"
   },
   "outputs": [],
   "source": [
    "#Cleaning the Train Dataset\n",
    "#train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
    "train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
    "train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub('[^a-zA-z\\s]','',x)) #removing all charecters except alphabets\n",
    "train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub(r'\\s+',' ',x)) #removing extra spaces\n",
    "train_df['reviews'] = train_df['reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIPVyIIYKNJq"
   },
   "outputs": [],
   "source": [
    "#Cleaning the test Dataset\n",
    "#test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
    "test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
    "test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub('[^a-zA-z\\s]','',x)) #removing all charecters except alphabets\n",
    "test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub(r'\\s+',' ',x)) #removing extra spaces\n",
    "test_df['reviews'] = test_df['reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FXKNMeEKVGm"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Creating_Embedding_matix\n",
    "sentences = train_df['reviews'].tolist() #converting reviews in dataframe to list.\n",
    "sentences = [line.lower().split(' ') for line in sentences] #Again convert each sentence in a list to list of lists. #Also each word to lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0oK-D45cjQR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "sentences - It should be list of lists\n",
    "min_count - minimum number of times a word should be present to create wordvectors\n",
    "window - number of words taken to train on either side of the word.\n",
    "size - embedding layer size\n",
    "workers - Number of cores it can parallely work on.\n",
    "sg - #1 for skipgram or #0 for Contionous Bag of words Model\n",
    "\"\"\"\n",
    "embed_model = Word2Vec(sentences, window=5, min_count=1, size=embedding_size, workers=2, sg=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cH5clMcWc6mm",
    "outputId": "fc2504ee-555e-420a-fa74-033ba159e4c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28591087, 30332070)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Embedding Matrix\n",
    "embed_model.train(sentences,total_examples=len(sentences),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0KF3J7ZjicN"
   },
   "outputs": [],
   "source": [
    "\"\"\"# save model\n",
    "embed_model.save('drive/My Drive/Movie_Reviews/RNN/skip_gram_model.bin')\n",
    "# load model\n",
    "skipgram_embed_model = Word2Vec.load('drive/My Drive/Movie_Reviews/RNN/skip_gram_model.bin')\"\"\"\n",
    "\n",
    "embed_model.save('drive/My Drive/Movie_Reviews/RNN/CBAG_model.bin')\n",
    "# load model\n",
    "embed_model = Word2Vec.load('drive/My Drive/Movie_Reviews/RNN/CBAG_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n_kP_kYnoZ-M",
    "outputId": "2dd950f0-d188-4f1e-a1f4-ea4ecd63a3c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=108991, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tv61JVLFpR8M"
   },
   "outputs": [],
   "source": [
    "\"\"\"The num_words in Tokenizer, the maximum number of words to keep, based on word frequency. \n",
    "If num_words = 10000 Only the most common 99999 words will be kept. all he extra words will be removed.\"\"\"\n",
    "tokenizer = Tokenizer(num_words = corpus_size) #setting up tokenizer\n",
    "b=tokenizer.fit_on_texts(train_df['reviews']) #fitting tokenizer on dataframe \n",
    "X_train = tokenizer.texts_to_sequences(train_df['reviews']) # removing least repeated words and converting them into sequence of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "z7dyDNV6rS30",
    "outputId": "8f3b11d5-f0f4-4c98-fa56-3c575fdf0308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 933\n",
      "Minimum review length: 3\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(len(max(X_train, key=len)))) #checking maximum review length\n",
    "print('Minimum review length: {}'.format(len(min(X_train, key=len)))) #checking minimum review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XrO9WjhYrwfb",
    "outputId": "111a10de-7e26-4003-fd35-bd8a9f506161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pads sequences to the same length.\n",
    "This function transforms a list of num_samples sequences (lists of integers) into a 2D Numpy array of \n",
    "shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the \n",
    "length of the longest sequence otherwise. Sequences that are shorter than num_timesteps are padded with value at the end.\n",
    "Sequences longer than num_timesteps are truncated so that they fit the desired length. \n",
    "The position where padding or truncation happens is determined by the arguments padding and truncating, respectively.\n",
    "Pre-padding is the default.\"\"\"\n",
    "\n",
    "X_train = pad_sequences(X_train,maxlen = sequence_length)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcWKrD7qu-P8"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((corpus_size, embedding_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27-LRsAKKpsr"
   },
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "\tembedding_vector = embed_model.wv[word]\n",
    "\tif embedding_vector is not None and i <=(corpus_size-1):\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "vYqGnEiDLQWU",
    "outputId": "0c23056c-d5c9-4206-ab0f-39b3ee3124a7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-adddb5c2101f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"idnt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix_50_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix_50_loaded' is not defined"
     ]
    }
   ],
   "source": [
    "word=\"idnt\"\n",
    "if word in list(embedding_matrix_50_loaded.index):\n",
    "  print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJLIIUFUFqv9"
   },
   "outputs": [],
   "source": [
    "#loading Glove model\n",
    "#File to Required Embedding Matrix\n",
    "embedding_matrix_50_loaded = pd.read_table(\"drive/My Drive/EmbeddingDownloaded/glove.6B.50d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if i <=(corpus_size-1):\n",
    "    if word in list(embedding_matrix_50_loaded.index):\n",
    "      embedding_vector = embedding_matrix_50_loaded.loc[word]\n",
    "      embedding_matrix[i] = np.asarray(embedding_vector)\n",
    "    else:\n",
    "      print (word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zIHkKg3yvknb",
    "outputId": "8418542b-1e6a-4fbf-8dd9-8be8e1f671eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cxs-gwaFyhPT"
   },
   "outputs": [],
   "source": [
    "Y_train = train_df['pos_or_neg']\n",
    "Y_test = test_df['pos_or_neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y4RsK2YPJn8W",
    "outputId": "98667730-9203-4491-9b11-fc545d999621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100) (5000, 100) (20000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train,Y_train, random_state=0, test_size=0.2)\n",
    "print(X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "F9n6A5kjy7p1",
    "outputId": "c8a9b41b-eaf8-4256-8e9c-2da5692f52a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4QFcC80JwfY"
   },
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(test_df['reviews']) #tokenizing the test data\n",
    "X_test = pad_sequences(X_test,maxlen = sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "k3FjQRDWGyl-",
    "outputId": "ce2c8395-5382-400b-9d61-01599df0700f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 50)           250000    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 433,553\n",
      "Trainable params: 183,553\n",
      "Non-trainable params: 250,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(input_dim=corpus_size,output_dim=embedding_size,input_length=sequence_length,trainable=False),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128)),\n",
    "  tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kaYkbP7UrP85"
   },
   "outputs": [],
   "source": [
    "#Declaring filepath for saving weigths\n",
    "filepath = \"weigths.best.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath,monitor='val_acc',verbose=1,save_best_only = True, mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "ty4xNanGsu_l",
    "outputId": "9f2a2688-174b-4dba-e9e1-2872bda1c84b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6892 - acc: 0.5636\n",
      "Epoch 00001: val_acc did not improve from 0.57720\n",
      "20000/20000 [==============================] - 54s 3ms/sample - loss: 0.6893 - acc: 0.5630 - val_loss: 0.6861 - val_acc: 0.5360\n",
      "Epoch 2/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6746 - acc: 0.5864\n",
      "Epoch 00002: val_acc did not improve from 0.57720\n",
      "20000/20000 [==============================] - 55s 3ms/sample - loss: 0.6747 - acc: 0.5860 - val_loss: 0.6811 - val_acc: 0.5688\n",
      "Epoch 3/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6692 - acc: 0.5954\n",
      "Epoch 00003: val_acc improved from 0.57720 to 0.58400, saving model to weigths.best.hdf5\n",
      "20000/20000 [==============================] - 53s 3ms/sample - loss: 0.6693 - acc: 0.5951 - val_loss: 0.6679 - val_acc: 0.5840\n",
      "Epoch 4/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6556 - acc: 0.6179\n",
      "Epoch 00004: val_acc improved from 0.58400 to 0.62400, saving model to weigths.best.hdf5\n",
      "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6556 - acc: 0.6180 - val_loss: 0.6522 - val_acc: 0.6240\n",
      "Epoch 5/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6441 - acc: 0.6326\n",
      "Epoch 00005: val_acc improved from 0.62400 to 0.62940, saving model to weigths.best.hdf5\n",
      "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6441 - acc: 0.6326 - val_loss: 0.6408 - val_acc: 0.6294\n",
      "Epoch 6/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.6319\n",
      "Epoch 00006: val_acc did not improve from 0.62940\n",
      "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6423 - acc: 0.6321 - val_loss: 0.6447 - val_acc: 0.6262\n",
      "Epoch 7/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6479 - acc: 0.6286\n",
      "Epoch 00007: val_acc did not improve from 0.62940\n",
      "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6476 - acc: 0.6289 - val_loss: 0.6527 - val_acc: 0.6190\n",
      "Epoch 8/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6364 - acc: 0.6403\n",
      "Epoch 00008: val_acc improved from 0.62940 to 0.63620, saving model to weigths.best.hdf5\n",
      "20000/20000 [==============================] - 53s 3ms/sample - loss: 0.6365 - acc: 0.6407 - val_loss: 0.6401 - val_acc: 0.6362\n",
      "Epoch 9/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6364 - acc: 0.6367\n",
      "Epoch 00009: val_acc did not improve from 0.63620\n",
      "20000/20000 [==============================] - 53s 3ms/sample - loss: 0.6361 - acc: 0.6374 - val_loss: 0.6417 - val_acc: 0.6352\n",
      "Epoch 10/10\n",
      "19800/20000 [============================>.] - ETA: 0s - loss: 0.6356 - acc: 0.6379\n",
      "Epoch 00010: val_acc did not improve from 0.63620\n",
      "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6357 - acc: 0.6380 - val_loss: 0.6470 - val_acc: 0.6312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff6fdd15f60>"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train, validation_data = [X_valid,Y_valid], epochs = 10, batch_size = 200, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qI2d9iZcu4Ud",
    "outputId": "1381fba5-f044-4f58-eb6e-b0ec2c98f762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 63.39600086212158\n"
     ]
    }
   ],
   "source": [
    "  scores = model.evaluate(X_test,Y_test,verbose=0)\n",
    "print(model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsBd3puE41LG"
   },
   "outputs": [],
   "source": [
    "os.mknod(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lqwpdQdxVqD"
   },
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\",'w') as json_file:\n",
    "  json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8v9hOkpG0bfz"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kIeySGJd5M2U",
    "outputId": "68301626-3506-4bb2-c13a-e4a6d51234a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load weights into new model\n",
    "loaded_model.load_weights(\"weigths.best.hdf5\")\n",
    "print(\"Loaded weigths from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y8lrZiBF5nO0",
    "outputId": "3535cb0b-c1d6-4ccd-d8ea-5784880fc2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 63.40%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HkHJ9L-o6SYT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_with_Word_Embeddings_tensorflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
