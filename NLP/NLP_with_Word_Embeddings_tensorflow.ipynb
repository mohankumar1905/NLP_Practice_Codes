{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_with_Word_Embeddings_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gqxtTIuEbWm",
        "colab_type": "code",
        "outputId": "ce1e0e9f-26a9-437e-b0c4-5bae2fc0a990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiNvBegZEwFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing libraries\n",
        "\n",
        "import numpy as np #for mathematical calculations if any needed \n",
        "import pandas as pd #for playing with dataframe\n",
        "import tensorflow as tf \n",
        "from sklearn.utils import shuffle\n",
        "from keras.preprocessing.text import Tokenizer #For tokenizing the words\n",
        "from keras.preprocessing.sequence import pad_sequences #for padding the words of same length\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv, os\n",
        "import re #for manipulating with regex\n",
        "import nltk #importing stopwords to be removed from the dataset\n",
        "from gensim.models import Word2Vec #for creating word embeddings using Word2Vec model(CBAG or Skipgram)/ Loading pretrained Word2Vec or Glove Model."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4WyLnQ0Fnhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing the dataset\n",
        "train_df = pd.read_csv(\"/content/drive/My Drive/Movie_Reviews/RNN/train_cleaned.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/My Drive/Movie_Reviews/RNN/test_cleaned.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_YAW-K4I8R2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input declearations \n",
        "embedding_size = 50\n",
        "sequence_length = 100\n",
        "corpus_size = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgly5eyWJLk-",
        "colab_type": "code",
        "outputId": "44c4d4a1-75e8-4282-f941-4092bc0a4bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#setting up stopwords from the nltk library\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWaxTaoVJhzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaning the Train Dataset\n",
        "#train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
        "train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
        "train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub('[^a-zA-z\\s]','',x)) #removing all charecters except alphabets\n",
        "train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub(r'\\s+',' ',x)) #removing extra spaces\n",
        "train_df['reviews'] = train_df['reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #removing stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIPVyIIYKNJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaning the test Dataset\n",
        "#test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
        "test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
        "test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub('[^a-zA-z\\s]','',x)) #removing all charecters except alphabets\n",
        "test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub(r'\\s+',' ',x)) #removing extra spaces\n",
        "test_df['reviews'] = test_df['reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #removing stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FXKNMeEKVGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Creating_Embedding_matix\n",
        "sentences = train_df['reviews'].tolist() #converting reviews in dataframe to list.\n",
        "sentences = [line.lower().split(' ') for line in sentences] #Again convert each sentence in a list to list of lists. #Also each word to lowercase "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0oK-D45cjQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "sentences - It should be list of lists\n",
        "min_count - minimum number of times a word should be present to create wordvectors\n",
        "window - number of words taken to train on either side of the word.\n",
        "size - embedding layer size\n",
        "workers - Number of cores it can parallely work on.\n",
        "sg - #1 for skipgram or #0 for Contionous Bag of words Model\n",
        "\"\"\"\n",
        "embed_model = Word2Vec(sentences, window=5, min_count=1, size=embedding_size, workers=2, sg=0) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH5clMcWc6mm",
        "colab_type": "code",
        "outputId": "fc2504ee-555e-420a-fa74-033ba159e4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Training Embedding Matrix\n",
        "embed_model.train(sentences,total_examples=len(sentences),epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28591087, 30332070)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0KF3J7ZjicN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"# save model\n",
        "embed_model.save('drive/My Drive/Movie_Reviews/RNN/skip_gram_model.bin')\n",
        "# load model\n",
        "skipgram_embed_model = Word2Vec.load('drive/My Drive/Movie_Reviews/RNN/skip_gram_model.bin')\"\"\"\n",
        "\n",
        "embed_model.save('drive/My Drive/Movie_Reviews/RNN/CBAG_model.bin')\n",
        "# load model\n",
        "embed_model = Word2Vec.load('drive/My Drive/Movie_Reviews/RNN/CBAG_model.bin')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_kP_kYnoZ-M",
        "colab_type": "code",
        "outputId": "2dd950f0-d188-4f1e-a1f4-ea4ecd63a3c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(embed_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=108991, size=50, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv61JVLFpR8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"The num_words in Tokenizer, the maximum number of words to keep, based on word frequency. \n",
        "If num_words = 10000 Only the most common 99999 words will be kept. all he extra words will be removed.\"\"\"\n",
        "tokenizer = Tokenizer(num_words = corpus_size) #setting up tokenizer\n",
        "b=tokenizer.fit_on_texts(train_df['reviews']) #fitting tokenizer on dataframe \n",
        "X_train = tokenizer.texts_to_sequences(train_df['reviews']) # removing least repeated words and converting them into sequence of numbers."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7dyDNV6rS30",
        "colab_type": "code",
        "outputId": "8f3b11d5-f0f4-4c98-fa56-3c575fdf0308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Maximum review length: {}'.format(len(max(X_train, key=len)))) #checking maximum review length\n",
        "print('Minimum review length: {}'.format(len(min(X_train, key=len)))) #checking minimum review length"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum review length: 933\n",
            "Minimum review length: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrO9WjhYrwfb",
        "colab_type": "code",
        "outputId": "111a10de-7e26-4003-fd35-bd8a9f506161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"Pads sequences to the same length.\n",
        "This function transforms a list of num_samples sequences (lists of integers) into a 2D Numpy array of \n",
        "shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the \n",
        "length of the longest sequence otherwise. Sequences that are shorter than num_timesteps are padded with value at the end.\n",
        "Sequences longer than num_timesteps are truncated so that they fit the desired length. \n",
        "The position where padding or truncation happens is determined by the arguments padding and truncating, respectively.\n",
        "Pre-padding is the default.\"\"\"\n",
        "\n",
        "X_train = pad_sequences(X_train,maxlen = sequence_length)\n",
        "print(X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcWKrD7qu-P8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((corpus_size, embedding_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27-LRsAKKpsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word, i in tokenizer.word_index.items():\n",
        "\tembedding_vector = embed_model.wv[word]\n",
        "\tif embedding_vector is not None and i <=(corpus_size-1):\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYqGnEiDLQWU",
        "colab_type": "code",
        "outputId": "0c23056c-d5c9-4206-ab0f-39b3ee3124a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "word=\"idnt\"\n",
        "if word in list(embedding_matrix_50_loaded.index):\n",
        "  print (word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-adddb5c2101f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"idnt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix_50_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix_50_loaded' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJLIIUFUFqv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading Glove model\n",
        "#File to Required Embedding Matrix\n",
        "embedding_matrix_50_loaded = pd.read_table(\"drive/My Drive/EmbeddingDownloaded/glove.6B.50d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if i <=(corpus_size-1):\n",
        "    if word in list(embedding_matrix_50_loaded.index):\n",
        "      embedding_vector = embedding_matrix_50_loaded.loc[word]\n",
        "      embedding_matrix[i] = np.asarray(embedding_vector)\n",
        "    else:\n",
        "      print (word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIHkKg3yvknb",
        "colab_type": "code",
        "outputId": "8418542b-1e6a-4fbf-8dd9-8be8e1f671eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxs-gwaFyhPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = train_df['pos_or_neg']\n",
        "Y_test = test_df['pos_or_neg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4RsK2YPJn8W",
        "colab_type": "code",
        "outputId": "98667730-9203-4491-9b11-fc545d999621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train,Y_train, random_state=0, test_size=0.2)\n",
        "print(X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 100) (5000, 100) (20000,) (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9n6A5kjy7p1",
        "colab_type": "code",
        "outputId": "c8a9b41b-eaf8-4256-8e9c-2da5692f52a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4QFcC80JwfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = tokenizer.texts_to_sequences(test_df['reviews']) #tokenizing the test data\n",
        "X_test = pad_sequences(X_test,maxlen = sequence_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3FjQRDWGyl-",
        "colab_type": "code",
        "outputId": "ce2c8395-5382-400b-9d61-01599df0700f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(input_dim=corpus_size,output_dim=embedding_size,input_length=sequence_length,trainable=False),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128)),\n",
        "  tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 100, 50)           250000    \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 256)               183296    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 433,553\n",
            "Trainable params: 183,553\n",
            "Non-trainable params: 250,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaYkbP7UrP85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Declaring filepath for saving weigths\n",
        "filepath = \"weigths.best.hdf5\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath,monitor='val_acc',verbose=1,save_best_only = True, mode = 'max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4xNanGsu_l",
        "colab_type": "code",
        "outputId": "9f2a2688-174b-4dba-e9e1-2872bda1c84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "model.fit(X_train,Y_train, validation_data = [X_valid,Y_valid], epochs = 10, batch_size = 200, callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6892 - acc: 0.5636\n",
            "Epoch 00001: val_acc did not improve from 0.57720\n",
            "20000/20000 [==============================] - 54s 3ms/sample - loss: 0.6893 - acc: 0.5630 - val_loss: 0.6861 - val_acc: 0.5360\n",
            "Epoch 2/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6746 - acc: 0.5864\n",
            "Epoch 00002: val_acc did not improve from 0.57720\n",
            "20000/20000 [==============================] - 55s 3ms/sample - loss: 0.6747 - acc: 0.5860 - val_loss: 0.6811 - val_acc: 0.5688\n",
            "Epoch 3/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6692 - acc: 0.5954\n",
            "Epoch 00003: val_acc improved from 0.57720 to 0.58400, saving model to weigths.best.hdf5\n",
            "20000/20000 [==============================] - 53s 3ms/sample - loss: 0.6693 - acc: 0.5951 - val_loss: 0.6679 - val_acc: 0.5840\n",
            "Epoch 4/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6556 - acc: 0.6179\n",
            "Epoch 00004: val_acc improved from 0.58400 to 0.62400, saving model to weigths.best.hdf5\n",
            "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6556 - acc: 0.6180 - val_loss: 0.6522 - val_acc: 0.6240\n",
            "Epoch 5/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6441 - acc: 0.6326\n",
            "Epoch 00005: val_acc improved from 0.62400 to 0.62940, saving model to weigths.best.hdf5\n",
            "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6441 - acc: 0.6326 - val_loss: 0.6408 - val_acc: 0.6294\n",
            "Epoch 6/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.6319\n",
            "Epoch 00006: val_acc did not improve from 0.62940\n",
            "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6423 - acc: 0.6321 - val_loss: 0.6447 - val_acc: 0.6262\n",
            "Epoch 7/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6479 - acc: 0.6286\n",
            "Epoch 00007: val_acc did not improve from 0.62940\n",
            "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6476 - acc: 0.6289 - val_loss: 0.6527 - val_acc: 0.6190\n",
            "Epoch 8/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6364 - acc: 0.6403\n",
            "Epoch 00008: val_acc improved from 0.62940 to 0.63620, saving model to weigths.best.hdf5\n",
            "20000/20000 [==============================] - 53s 3ms/sample - loss: 0.6365 - acc: 0.6407 - val_loss: 0.6401 - val_acc: 0.6362\n",
            "Epoch 9/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6364 - acc: 0.6367\n",
            "Epoch 00009: val_acc did not improve from 0.63620\n",
            "20000/20000 [==============================] - 53s 3ms/sample - loss: 0.6361 - acc: 0.6374 - val_loss: 0.6417 - val_acc: 0.6352\n",
            "Epoch 10/10\n",
            "19800/20000 [============================>.] - ETA: 0s - loss: 0.6356 - acc: 0.6379\n",
            "Epoch 00010: val_acc did not improve from 0.63620\n",
            "20000/20000 [==============================] - 52s 3ms/sample - loss: 0.6357 - acc: 0.6380 - val_loss: 0.6470 - val_acc: 0.6312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff6fdd15f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI2d9iZcu4Ud",
        "colab_type": "code",
        "outputId": "1381fba5-f044-4f58-eb6e-b0ec2c98f762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  scores = model.evaluate(X_test,Y_test,verbose=0)\n",
        "print(model.metrics_names[1], scores[1]*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc 63.39600086212158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsBd3puE41LG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mknod(\"model.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lqwpdQdxVqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the model\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\",'w') as json_file:\n",
        "  json_file.write(model_json)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v9hOkpG0bfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIeySGJd5M2U",
        "colab_type": "code",
        "outputId": "68301626-3506-4bb2-c13a-e4a6d51234a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load weights into new model\n",
        "loaded_model.load_weights(\"weigths.best.hdf5\")\n",
        "print(\"Loaded weigths from disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8lrZiBF5nO0",
        "colab_type": "code",
        "outputId": "3535cb0b-c1d6-4ccd-d8ea-5784880fc2c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 63.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkHJ9L-o6SYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}