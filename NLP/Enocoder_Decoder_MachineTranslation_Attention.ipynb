{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4352,
     "status": "ok",
     "timestamp": 1558897135745,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "N7vahz90TImD",
    "outputId": "6c5c2d2d-aa6f-4218-8e52-5b5bb519457e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1996,
     "status": "ok",
     "timestamp": 1558897138659,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "N00-Z3d1TyMZ",
    "outputId": "230f3578-9b25-4dcc-a99d-e62078fd2bcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/content/drive/My Drive/Deep Learning/deueng/deu.txt\",encoding='utf-8', delimiter = r'\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WoIKFt_aUY5E"
   },
   "outputs": [],
   "source": [
    "df.columns = ['English', 'German']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GyIfoVqaZlD-"
   },
   "outputs": [],
   "source": [
    "df['German'] = df['German'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmPdieriaNE1"
   },
   "outputs": [],
   "source": [
    "df['English']  = df['English'].str.replace(\"won't\",\"will not\")\n",
    "df['English']  = df['English'].str.replace(\"ain't\",\"am not\")\n",
    "df['English']  = df['English'].str.replace(\"'s\",\" is\")\n",
    "df['English']  = df['English'].str.replace(\"'m\",\" am\")\n",
    "df['English']  = df['English'].str.replace(\"'re'\",\" are\")\n",
    "df['English']  = df['English'].str.replace(\"can't\",\"can not\")\n",
    "df['English']  = df['English'].str.replace(\"'ll\",\" will\")\n",
    "df['English']  = df['English'].str.replace(\"n't\",\" not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLGm03TOeVn4"
   },
   "outputs": [],
   "source": [
    "df['German'] = df['German'].str.replace(\"'s\",\" es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TFCgIZtrkjR5"
   },
   "outputs": [],
   "source": [
    "df['English'] = df['English'].map(lambda x: x.lower())\n",
    "df['German'] = df['German'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyXaunHhpAFz"
   },
   "outputs": [],
   "source": [
    "df['English'] = df['English'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x)) #removing all charecters except alphabets\n",
    "df['English'] = df['English'].apply(lambda x: re.sub(r'\\s+',' ',x)) #removing extra spaces\n",
    "df['English'] = df['English'].map(lambda x: ' '.join(word for word in x.split() if len(word)>1)) #removing single charecters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lbc3L8kqp4xH"
   },
   "outputs": [],
   "source": [
    "df['German'] = df['German'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x)) #removing all charecters except alphabets\n",
    "df['German'] = df['German'].apply(lambda x: re.sub(r'\\s+',' ',x)) #removing extra spaces\n",
    "df['German'] = df['German'].map(lambda x: ' '.join(word for word in x.split() if len(word)>1)) #removing single charecters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1973,
     "status": "ok",
     "timestamp": 1558897147589,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "6rXoLcNqp8QP",
    "outputId": "bb34582d-6dfe-434c-da43-b10d92b9fac4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call 110 right now'"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['English'][22519]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 859,
     "status": "ok",
     "timestamp": 1558897147590,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "GEzAWJy8fkmt",
    "outputId": "d7546fc3-a6c0-4453-a829-d7cfc32c320c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192881, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8uDLBcJjvwRa"
   },
   "outputs": [],
   "source": [
    "df = df[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAS7Z605wQl_"
   },
   "outputs": [],
   "source": [
    "df_train = df[:9000]\n",
    "df_test = df[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1558897150061,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "PMAkWobrrnQb",
    "outputId": "e70fca71-543c-4fc8-f47f-2abe06aaecd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2205\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "english_tokenizer = Tokenizer()\n",
    "english_tokenizer.fit_on_texts(df['English'])\n",
    "eng_vocab_size = len(english_tokenizer.word_index)+1\n",
    "print (eng_vocab_size)\n",
    "print (max([len(x) for x in df['English'].str.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8Zq1Axis3qx"
   },
   "outputs": [],
   "source": [
    "eng_max_len = (max([len(x) for x in df['English'].str.split()]))\n",
    "eng_max_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1558897152798,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "BuMjIzVPuKpJ",
    "outputId": "5b5b1c9c-ad67-4087-cf51-91f27bbeeb48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3565\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "german_tokenizer = Tokenizer()\n",
    "german_tokenizer.fit_on_texts(df['German'])\n",
    "german_vocab_size = len(german_tokenizer.word_index)+1\n",
    "print (german_vocab_size)\n",
    "print (max([len(x) for x in df['German'].str.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIvwvSE8s9EO"
   },
   "outputs": [],
   "source": [
    "german_max_len = (max([len(x) for x in df['German'].str.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iY-DM0wMgxBO"
   },
   "outputs": [],
   "source": [
    "trainX = german_tokenizer.texts_to_sequences(df_train['German'])\n",
    "trainX = pad_sequences(trainX, maxlen=german_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1558897156686,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "yamkBJlZxGGx",
    "outputId": "cfed09c7-7598-4546-901b-68866d54a7d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AksYjEzff2a5"
   },
   "outputs": [],
   "source": [
    "trainY = english_tokenizer.texts_to_sequences(df_train['English'])\n",
    "trainY = pad_sequences(trainY, maxlen=eng_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1558897159624,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "ePDA9c0EsGSM",
    "outputId": "40a08de0-3c1f-4253-b8cf-d92b6c3d8dfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_9gH8yAqwoh"
   },
   "outputs": [],
   "source": [
    "trainY = to_categorical(trainY, num_classes=eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1558897167507,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "3D7EyUpIq5zC",
    "outputId": "c68412e8-3a07-4915-bdc0-a4e562444fbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10, 2205)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 904,
     "status": "ok",
     "timestamp": 1558897175308,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "euJEaMGmxCPG",
    "outputId": "54efcc99-4168-4a77-fbbc-cf41784b6679"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10, 2205)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX = german_tokenizer.texts_to_sequences(df_test['German'])\n",
    "testX = pad_sequences(testX, maxlen=german_max_len, padding='post')\n",
    "testY = english_tokenizer.texts_to_sequences(df_test['English'])\n",
    "testY = pad_sequences(testY, maxlen=eng_max_len, padding='post')\n",
    "testY = to_categorical(testY, num_classes=eng_vocab_size)\n",
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSCgDTdnvTsE"
   },
   "outputs": [],
   "source": [
    "embedding_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uy4FnvphT43E"
   },
   "outputs": [],
   "source": [
    "def time_distributed_dense(x, w, b=None, dropout=None,\n",
    "                           input_dim=None, output_dim=None, timesteps=None):\n",
    "    '''Apply y.w + b for every temporal slice y of x.\n",
    "    '''\n",
    "    if not input_dim:\n",
    "        # won't work with TensorFlow\n",
    "        input_dim = K.shape(x)[2]\n",
    "    if not timesteps:\n",
    "        # won't work with TensorFlow\n",
    "        timesteps = K.shape(x)[1]\n",
    "    if not output_dim:\n",
    "        # won't work with TensorFlow\n",
    "        output_dim = K.shape(w)[1]\n",
    "\n",
    "    if dropout:\n",
    "        # apply the same dropout pattern at every timestep\n",
    "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
    "        dropout_matrix = K.dropout(ones, dropout)\n",
    "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
    "        x *= expanded_dropout_matrix\n",
    "\n",
    "    # collapse time dimension and batch dimension together\n",
    "    x = K.reshape(x, (-1, input_dim))\n",
    "\n",
    "    x = K.dot(x, w)\n",
    "    if b:\n",
    "        x = x + b\n",
    "    # reshape to 3D tensor\n",
    "    x = K.reshape(x, (-1, timesteps, output_dim))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipjvViXiryMJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.layers import TimeDistributed, Dense\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
    "\n",
    "class AttentionDecoder(Recurrent):\n",
    "\n",
    "    def __init__(self, units, output_dim,\n",
    "                 activation='tanh',\n",
    "                 return_probabilities=False,\n",
    "                 name='AttentionDecoder',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 recurrent_initializer='orthogonal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
    "        encoder and outputs the decoded states\n",
    "        :param units: dimension of the hidden state and the attention matrices\n",
    "        :param output_dim: the number of labels in the output space\n",
    "\n",
    "        references:\n",
    "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
    "            \"Neural machine translation by jointly learning to align and translate.\"\n",
    "            arXiv preprint arXiv:1409.0473 (2014).\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.return_probabilities = return_probabilities\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "        self.name = name\n",
    "        self.return_sequences = True  # must return sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
    "          for model details that correspond to the matrices here.\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
    "\n",
    "        if self.stateful:\n",
    "            super(AttentionDecoder, self).reset_states()\n",
    "\n",
    "        self.states = [None, None]  # y, s\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for creating the context vector\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='V_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='W_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='U_a',\n",
    "                                   initializer=self.kernel_initializer,\n",
    "                                   regularizer=self.kernel_regularizer,\n",
    "                                   constraint=self.kernel_constraint)\n",
    "        self.b_a = self.add_weight(shape=(self.units,),\n",
    "                                   name='b_a',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the r (reset) gate\n",
    "        \"\"\"\n",
    "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_r',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_r = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_r',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        \"\"\"\n",
    "            Matrices for the z (update) gate\n",
    "        \"\"\"\n",
    "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_z',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_z = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_z',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for the proposal\n",
    "        \"\"\"\n",
    "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='C_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
    "                                   name='U_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
    "                                   name='W_p',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_p = self.add_weight(shape=(self.units, ),\n",
    "                                   name='b_p',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "        \"\"\"\n",
    "            Matrices for making the final prediction vector\n",
    "        \"\"\"\n",
    "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
    "                                   name='C_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
    "                                   name='U_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
    "                                   name='W_o',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
    "                                   name='b_o',\n",
    "                                   initializer=self.bias_initializer,\n",
    "                                   regularizer=self.bias_regularizer,\n",
    "                                   constraint=self.bias_constraint)\n",
    "\n",
    "        # For creating the initial state:\n",
    "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
    "                                   name='W_s',\n",
    "                                   initializer=self.recurrent_initializer,\n",
    "                                   regularizer=self.recurrent_regularizer,\n",
    "                                   constraint=self.recurrent_constraint)\n",
    "\n",
    "        self.input_spec = [\n",
    "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
    "        self.x_seq = x\n",
    "\n",
    "        # apply the a dense layer over the time dimension of the sequence\n",
    "        # do it here because it doesn't depend on any previous steps\n",
    "        # thefore we can save computation time:\n",
    "        self._uxpb = time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
    "                                             input_dim=self.input_dim,\n",
    "                                             timesteps=self.timesteps,\n",
    "                                             output_dim=self.units)\n",
    "\n",
    "        return super(AttentionDecoder, self).call(x)\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        # apply the matrix on the first time step to get the initial s0.\n",
    "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
    "\n",
    "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
    "        # output_dim)\n",
    "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
    "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
    "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
    "        y0 = K.tile(y0, [1, self.output_dim])\n",
    "\n",
    "        return [y0, s0]\n",
    "\n",
    "    def step(self, x, states):\n",
    "\n",
    "        ytm, stm = states\n",
    "\n",
    "        # repeat the hidden state to the length of the sequence\n",
    "        _stm = K.repeat(stm, self.timesteps)\n",
    "\n",
    "        # now multiplty the weight matrix with the repeated hidden state\n",
    "        _Wxstm = K.dot(_stm, self.W_a)\n",
    "\n",
    "        # calculate the attention probabilities\n",
    "        # this relates how much other timesteps contributed to this one.\n",
    "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
    "                   K.expand_dims(self.V_a))\n",
    "        at = K.exp(et)\n",
    "        at_sum = K.sum(at, axis=1)\n",
    "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
    "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
    "\n",
    "        # calculate the context vector\n",
    "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
    "        # ~~~> calculate new hidden state\n",
    "        # first calculate the \"r\" gate:\n",
    "\n",
    "        rt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_r)\n",
    "            + K.dot(stm, self.U_r)\n",
    "            + K.dot(context, self.C_r)\n",
    "            + self.b_r)\n",
    "\n",
    "        # now calculate the \"z\" gate\n",
    "        zt = activations.sigmoid(\n",
    "            K.dot(ytm, self.W_z)\n",
    "            + K.dot(stm, self.U_z)\n",
    "            + K.dot(context, self.C_z)\n",
    "            + self.b_z)\n",
    "\n",
    "        # calculate the proposal hidden state:\n",
    "        s_tp = activations.tanh(\n",
    "            K.dot(ytm, self.W_p)\n",
    "            + K.dot((rt * stm), self.U_p)\n",
    "            + K.dot(context, self.C_p)\n",
    "            + self.b_p)\n",
    "\n",
    "        # new hidden state:\n",
    "        st = (1-zt)*stm + zt * s_tp\n",
    "\n",
    "        yt = activations.softmax(\n",
    "            K.dot(ytm, self.W_o)\n",
    "            + K.dot(stm, self.U_o)\n",
    "            + K.dot(context, self.C_o)\n",
    "            + self.b_o)\n",
    "\n",
    "        if self.return_probabilities:\n",
    "            return at, [yt, st]\n",
    "        else:\n",
    "            return yt, [yt, st]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "            For Keras internal compatability checking\n",
    "        \"\"\"\n",
    "        if self.return_probabilities:\n",
    "            return (None, self.timesteps, self.timesteps)\n",
    "        else:\n",
    "            return (None, self.timesteps, self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "            For rebuilding models on load time.\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'units': self.units,\n",
    "            'return_probabilities': self.return_probabilities\n",
    "        }\n",
    "        base_config = super(AttentionDecoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8480,
     "status": "ok",
     "timestamp": 1558897192132,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "AnJpRSccr7GM",
    "outputId": "888b3392-610b-44c6-d9a0-4f2682acb909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 256)           912640    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 100)           142800    \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 10, 2205)          6057230   \n",
      "=================================================================\n",
      "Total params: 7,112,670\n",
      "Trainable params: 7,112,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(german_vocab_size,embedding_size,input_length=german_max_len))\n",
    "model.add(LSTM(100,return_sequences=True))\n",
    "model.add(AttentionDecoder(100, eng_vocab_size))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3TIlyF8watj"
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 134688,
     "status": "ok",
     "timestamp": 1558897330091,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "RVKS5Dc0xuFE",
    "outputId": "dd99872a-efe6-4440-b52a-acc09c60edfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      " - 8s - loss: 3.2227 - val_loss: 2.1422\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.14217, saving model to model.h5\n",
      "Epoch 2/30\n",
      " - 4s - loss: 1.6074 - val_loss: 2.0690\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.14217 to 2.06901, saving model to model.h5\n",
      "Epoch 3/30\n",
      " - 4s - loss: 1.5311 - val_loss: 2.0278\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.06901 to 2.02778, saving model to model.h5\n",
      "Epoch 4/30\n",
      " - 4s - loss: 1.4613 - val_loss: 1.9931\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.02778 to 1.99307, saving model to model.h5\n",
      "Epoch 5/30\n",
      " - 4s - loss: 1.3988 - val_loss: 1.9761\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.99307 to 1.97611, saving model to model.h5\n",
      "Epoch 6/30\n",
      " - 4s - loss: 1.3522 - val_loss: 1.9300\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.97611 to 1.92999, saving model to model.h5\n",
      "Epoch 7/30\n",
      " - 4s - loss: 1.3067 - val_loss: 1.9021\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.92999 to 1.90211, saving model to model.h5\n",
      "Epoch 8/30\n",
      " - 4s - loss: 1.2573 - val_loss: 1.8583\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.90211 to 1.85826, saving model to model.h5\n",
      "Epoch 9/30\n",
      " - 4s - loss: 1.2044 - val_loss: 1.8362\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.85826 to 1.83622, saving model to model.h5\n",
      "Epoch 10/30\n",
      " - 4s - loss: 1.1532 - val_loss: 1.8146\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.83622 to 1.81457, saving model to model.h5\n",
      "Epoch 11/30\n",
      " - 4s - loss: 1.1083 - val_loss: 1.8060\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.81457 to 1.80604, saving model to model.h5\n",
      "Epoch 12/30\n",
      " - 4s - loss: 1.0689 - val_loss: 1.7816\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.80604 to 1.78158, saving model to model.h5\n",
      "Epoch 13/30\n",
      " - 4s - loss: 1.0301 - val_loss: 1.7444\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.78158 to 1.74440, saving model to model.h5\n",
      "Epoch 14/30\n",
      " - 4s - loss: 0.9901 - val_loss: 1.7208\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.74440 to 1.72076, saving model to model.h5\n",
      "Epoch 15/30\n",
      " - 4s - loss: 0.9455 - val_loss: 1.7067\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.72076 to 1.70671, saving model to model.h5\n",
      "Epoch 16/30\n",
      " - 4s - loss: 0.9045 - val_loss: 1.7034\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.70671 to 1.70335, saving model to model.h5\n",
      "Epoch 17/30\n",
      " - 4s - loss: 0.8606 - val_loss: 1.6612\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.70335 to 1.66115, saving model to model.h5\n",
      "Epoch 18/30\n",
      " - 4s - loss: 0.8193 - val_loss: 1.6742\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.66115\n",
      "Epoch 19/30\n",
      " - 4s - loss: 0.7789 - val_loss: 1.6819\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.66115\n",
      "Epoch 20/30\n",
      " - 4s - loss: 0.7405 - val_loss: 1.6468\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.66115 to 1.64678, saving model to model.h5\n",
      "Epoch 21/30\n",
      " - 4s - loss: 0.7024 - val_loss: 1.6424\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.64678 to 1.64244, saving model to model.h5\n",
      "Epoch 22/30\n",
      " - 4s - loss: 0.6617 - val_loss: 1.6214\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.64244 to 1.62140, saving model to model.h5\n",
      "Epoch 23/30\n",
      " - 4s - loss: 0.6241 - val_loss: 1.6258\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.62140\n",
      "Epoch 24/30\n",
      " - 4s - loss: 0.5877 - val_loss: 1.6195\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.62140 to 1.61954, saving model to model.h5\n",
      "Epoch 25/30\n",
      " - 4s - loss: 0.5509 - val_loss: 1.5951\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.61954 to 1.59506, saving model to model.h5\n",
      "Epoch 26/30\n",
      " - 4s - loss: 0.5170 - val_loss: 1.6019\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.59506\n",
      "Epoch 27/30\n",
      " - 4s - loss: 0.4846 - val_loss: 1.6310\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.59506\n",
      "Epoch 28/30\n",
      " - 4s - loss: 0.4508 - val_loss: 1.6209\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.59506\n",
      "Epoch 29/30\n",
      " - 4s - loss: 0.4201 - val_loss: 1.5976\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.59506\n",
      "Epoch 30/30\n",
      " - 4s - loss: 0.3909 - val_loss: 1.5934\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.59506 to 1.59339, saving model to model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe17dce1780>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=30, batch_size=128, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1558897334234,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "u8hnobbgxwt-",
    "outputId": "736afeb0-fde5-4f74-ecd2-342a249d10f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tom'"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishword_for_id = dict((v,k) for k,v in english_tokenizer.word_index.items())  \n",
    "englishword_for_id[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1558897637760,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "2Ynq0Lhkb3Or",
    "outputId": "0a19e014-c348-4a5a-cc09-fe31fbf8167a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saw\n",
      "that\n"
     ]
    }
   ],
   "source": [
    "for word in np.argmax(model.predict(pad_sequences(german_tokenizer.texts_to_sequences([\"ich habe gestern einen Film gesehen\"]),padding=\"post\",maxlen=10)),axis=2)[0]:\n",
    "  if word != 0:\n",
    "    print (englishword_for_id[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1558246961980,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "_riH_eivccHh",
    "outputId": "060a5183-2d2c-4fe9-a1fe-f99ef5dce503"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1165,  424,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(german_tokenizer.texts_to_sequences([\"Guten Morgen Jagan\"]),padding=\"post\",maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1558897674670,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "BH63xfkIcpjI",
    "outputId": "9031484e-c254-425e-912f-b1ab7cc41b3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "predict_sentence_split = []\n",
    "for word in np.argmax(model.predict(pad_sequences(german_tokenizer.texts_to_sequences([df['German'][9999]]),padding=\"post\",maxlen=10)),axis=2)[0]:\n",
    "  if word != 0:\n",
    "    print (englishword_for_id[word])\n",
    "    predict_sentence_split.append(englishword_for_id[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "axMPr2VAegeU"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1558897686295,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "h9v6GPm9plQA",
    "outputId": "a4c3984d-ed33-4b8e-c3eb-34e3a7bfcb54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just found it'"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['English'][9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1558897690308,
     "user": {
      "displayName": "Mohankumar Balasubramaniyam",
      "photoUrl": "https://lh4.googleusercontent.com/-WDGyxwVw0Dw/AAAAAAAAAAI/AAAAAAAAMXg/s0XKgKv-dN8/s64/photo.jpg",
      "userId": "03872728872081242131"
     },
     "user_tz": -330
    },
    "id": "u56OfuqaxjEw",
    "outputId": "7601eda2-5bad-43ed-adb2-32d36d33fbc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6065306597126334"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu([df['English'][9999].split()],predict_sentence_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdDjH7w-yXJH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Enocoder_Decoder_MachineTranslation_Attention.ipynb",
   "provenance": [
    {
     "file_id": "1JGg2_NQiTD0rluY1Xe-yq4ISAEri-CMX",
     "timestamp": 1558883261330
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
