{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "ZiNvBegZEwFq",
    "outputId": "c9fbf1cd-3dec-47ef-b0f1-afcb126fc907"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "\n",
    "import numpy as np #for mathematical calculations if any needed \n",
    "import pandas as pd #for playing with dataframe\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer #For tokenizing the words\n",
    "from keras.preprocessing.sequence import pad_sequences #for padding the words of same length\n",
    "from keras.models import Sequential #for building a sequence models\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense #for defining a layers inside the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import re #for manipulating with regex\n",
    "import nltk #importing stopwords to be removed from the dataset\n",
    "from gensim.models import Word2Vec #for creating word embeddings using Word2Vec model(CBAG or Skipgram)/ Loading pretrained Word2Vec or Glove Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4WyLnQ0Fnhd"
   },
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "train_df = pd.read_csv(\"/content/drive/My Drive/Movie_Reviews/RNN/train_cleaned.csv\")\n",
    "test_df = pd.read_csv(\"/content/drive/My Drive/Movie_Reviews/RNN/test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_YAW-K4I8R2"
   },
   "outputs": [],
   "source": [
    "#input declearations \n",
    "embedding_size = 50\n",
    "sequence_length = 100\n",
    "corpus_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "Rgly5eyWJLk-",
    "outputId": "a68d32c3-fac6-4676-edeb-63930ead2026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#setting up stopwords from the nltk library\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWaxTaoVJhzr"
   },
   "outputs": [],
   "source": [
    "#Cleaning the Train Dataset\n",
    "#train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
    "train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
    "train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub('[^a-zA-z\\s]','',x)) #removing all charecters except alphabets\n",
    "train_df['reviews'] = train_df['reviews'].apply(lambda x: re.sub(r'\\s+',' ',x)) #removing extra spaces\n",
    "train_df['reviews'] = train_df['reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIPVyIIYKNJq"
   },
   "outputs": [],
   "source": [
    "#Cleaning the test Dataset\n",
    "#test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
    "test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub('<br />',' ',x)) #removing nextline. If it is in another format, please check and remove it.\n",
    "test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub('[^a-zA-z\\s]','',x)) #removing all charecters except alphabets\n",
    "test_df['reviews'] = test_df['reviews'].apply(lambda x: re.sub(r'\\s+',' ',x)) #removing extra spaces\n",
    "test_df['reviews'] = test_df['reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FXKNMeEKVGm"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Creating_Embedding_matix\n",
    "sentences = train_df['reviews'].tolist() #converting reviews in dataframe to list.\n",
    "sentences = [line.lower().split(' ') for line in sentences] #Again convert each sentence in a list to list of lists. #Also each word to lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0oK-D45cjQR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "sentences - It should be list of lists\n",
    "min_count - minimum number of times a word should be present to create wordvectors\n",
    "window - number of words taken to train on either side of the word.\n",
    "size - embedding layer size\n",
    "workers - Number of cores it can parallely work on.\n",
    "sg - #1 for skipgram or #0 for Contionous Bag of words Model\n",
    "\"\"\"\n",
    "embed_model = Word2Vec(sentences, window=5, min_count=1, size=embedding_size, workers=2, sg=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cH5clMcWc6mm",
    "outputId": "c926cbf9-e8d9-4fde-d459-e5e3e0874903"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28591087, 30332070)"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Embedding Matrix\n",
    "embed_model.train(sentences,total_examples=len(sentences),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "K0KF3J7ZjicN",
    "outputId": "d7395257-80b5-4791-c520-829fe33ca6b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# save model\n",
    "embed_model.save('drive/My Drive/Movie_Reviews/RNN/skip_gram_model.bin')\n",
    "# load model\n",
    "skipgram_embed_model = Word2Vec.load('drive/My Drive/Movie_Reviews/RNN/skip_gram_model.bin')\"\"\"\n",
    "\n",
    "embed_model.save('drive/My Drive/Movie_Reviews/RNN/CBAG_model.bin')\n",
    "# load model\n",
    "embed_model = Word2Vec.load('drive/My Drive/Movie_Reviews/RNN/CBAG_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "n_kP_kYnoZ-M",
    "outputId": "c804b74a-495d-4fc7-aa98-eb68016dd0a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=108991, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tv61JVLFpR8M"
   },
   "outputs": [],
   "source": [
    "\"\"\"The num_words in Tokenizer, the maximum number of words to keep, based on word frequency. \n",
    "If num_words = 10000 Only the most common 9999 words will be kept. all he extra words will be removed.\"\"\"\n",
    "tokenizer = Tokenizer(num_words = corpus_size) #setting up tokenizer\n",
    "b=tokenizer.fit_on_texts(train_df['reviews']) #fitting tokenizer on dataframe \n",
    "X_train = tokenizer.texts_to_sequences(train_df['reviews']) # removing least repeated words and converting them into sequence of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "z7dyDNV6rS30",
    "outputId": "d0171d1a-a2ca-4c9a-a8a9-eb4947a82b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 933\n",
      "Minimum review length: 3\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(len(max(X_train, key=len)))) #checking maximum review length\n",
    "print('Minimum review length: {}'.format(len(min(X_train, key=len)))) #checking minimum review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "XrO9WjhYrwfb",
    "outputId": "415e1bfe-98db-471a-dc4f-48cf465a40cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pads sequences to the same length.\n",
    "This function transforms a list of num_samples sequences (lists of integers) into a 2D Numpy array of \n",
    "shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the \n",
    "length of the longest sequence otherwise. Sequences that are shorter than num_timesteps are padded with value at the end.\n",
    "Sequences longer than num_timesteps are truncated so that they fit the desired length. \n",
    "The position where padding or truncation happens is determined by the arguments padding and truncating, respectively.\n",
    "Pre-padding is the default.\"\"\"\n",
    "\n",
    "X_train = pad_sequences(X_train,maxlen = sequence_length)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcWKrD7qu-P8"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((corpus_size, embedding_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27-LRsAKKpsr"
   },
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "\tembedding_vector = embed_model.wv[word]\n",
    "\tif embedding_vector is not None and i <=(corpus_size-1):\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "vYqGnEiDLQWU",
    "outputId": "842e1fc6-bc5d-4ef4-affa-46e6b4ad793e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-adddb5c2101f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"idnt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix_50_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix_50_loaded' is not defined"
     ]
    }
   ],
   "source": [
    "word=\"idnt\"\n",
    "if word in list(embedding_matrix_50_loaded.index):\n",
    "  print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1598
    },
    "colab_type": "code",
    "id": "DJLIIUFUFqv9",
    "outputId": "09840592-2b78-41a1-8bca-6c47657e90fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itbr\n",
      "moviebr\n",
      "filmbr\n",
      "timebr\n",
      "clichs\n",
      "lowbudget\n",
      "thembr\n",
      "theyve\n",
      "himbr\n",
      "thisbr\n",
      "allbr\n",
      "storybr\n",
      "onebr\n",
      "thatbr\n",
      "mebr\n",
      "wellbr\n",
      "yearold\n",
      "overthetop\n",
      "wouldve\n",
      "outbr\n",
      "waybr\n",
      "herebr\n",
      "isbr\n",
      "bmovie\n",
      "clichd\n",
      "lifebr\n",
      "couldve\n",
      "endbr\n",
      "writerdirector\n",
      "onbr\n",
      "againbr\n",
      "goodbr\n",
      "therebr\n",
      "youbr\n",
      "badbr\n",
      "oneliners\n",
      "toobr\n",
      "herbr\n",
      "mstk\n",
      "moviesbr\n",
      "theyd\n",
      "betterbr\n",
      "reallife\n",
      "mustsee\n",
      "theyll\n",
      "worldbr\n",
      "filmsbr\n",
      "funnybr\n",
      "upbr\n",
      "charactersbr\n",
      "spoilersbr\n",
      "timesbr\n",
      "workbr\n",
      "showbr\n",
      "watchbr\n",
      "itll\n",
      "morebr\n",
      "notbr\n",
      "everyones\n",
      "scenebr\n",
      "seenbr\n",
      "etcbr\n",
      "screenbr\n",
      "wasbr\n",
      "scenesbr\n",
      "madefortv\n",
      "onedimensional\n",
      "inbr\n",
      "bebr\n",
      "peoplebr\n",
      "muchbr\n",
      "seriesbr\n",
      "plotbr\n",
      "eitherbr\n",
      "offbr\n",
      "aboutbr\n",
      "minutesbr\n",
      "funbr\n",
      "withbr\n",
      "middleaged\n",
      "daybr\n",
      "otherbr\n",
      "wrongbr\n",
      "actingbr\n",
      "yearsbr\n",
      "nowbr\n",
      "audiencebr\n",
      "characterbr\n",
      "thingbr\n",
      "madebr\n",
      "wellwritten\n",
      "thoughtprovoking\n",
      "togetherbr\n"
     ]
    }
   ],
   "source": [
    "#loading Glove model\n",
    "#File to Required Embedding Matrix\n",
    "embedding_matrix_50_loaded = pd.read_table(\"drive/My Drive/EmbeddingDownloaded/glove.6B.50d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if i <=(corpus_size-1):\n",
    "    if word in list(embedding_matrix_50_loaded.index):\n",
    "      embedding_vector = embedding_matrix_50_loaded.loc[word]\n",
    "      embedding_matrix[i] = np.asarray(embedding_vector)\n",
    "    else:\n",
    "      print (word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "zIHkKg3yvknb",
    "outputId": "5f501da5-49dc-4af5-8b1e-5987dba54c18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cxs-gwaFyhPT"
   },
   "outputs": [],
   "source": [
    "Y_train = train_df['pos_or_neg']\n",
    "Y_test = test_df['pos_or_neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "y4RsK2YPJn8W",
    "outputId": "078fe8df-17c1-4fa8-a05a-f0c6c22d8135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100) (5000, 100) (20000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train,Y_train, random_state=0, test_size=0.2)\n",
    "print(X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "F9n6A5kjy7p1",
    "outputId": "3f2e1bb9-15e3-4755-ca8b-2f1143be4d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4QFcC80JwfY"
   },
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(test_df['reviews']) #tokenizing the test data\n",
    "X_test = pad_sequences(X_test,maxlen = sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "r5CU8bSIEQOL",
    "outputId": "5f2a2e3b-62e2-4e4d-bdc0-9301754c061e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Building the model\n",
    "model=Sequential()\n",
    "#model.add(Embedding(input_dim=corpus_size,output_dim=embedding_size\"\"\",weights=[embedding_matrix],\"\"\",input_length=sequence_length,trainable=True))\n",
    "model.add(Embedding(input_dim=corpus_size,output_dim=embedding_size,input_length=sequence_length,trainable=True))\n",
    "model.add(Bidirectional(LSTM(units=128)))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "4HrXkSZHgx4-",
    "outputId": "08965beb-dd12-45ed-bb22-8b5e2b25dc53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "n1oxnkd7FNqH",
    "outputId": "4fd9bd9c-7986-47ed-fcc3-62311336164f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 50)           250000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               183296    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 433,553\n",
      "Trainable params: 433,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kg7Hy1hlKEFA"
   },
   "outputs": [],
   "source": [
    "#Declaring filepath for saving weigths\n",
    "filepath = \"weigths.best.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYBAIlBcL8Oy"
   },
   "outputs": [],
   "source": [
    "#Declaring filepath for saving weigths\n",
    "filepath = \"weigths.best.hdf5\"\n",
    "\n",
    "#creating a checkpoint\n",
    "checkpoint = ModelCheckpoint(filepath,monitor='val_acc',verbose=1,save_best_only = True, mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JApq-qI0L_3O",
    "outputId": "ca021dd5-1325-42ac-b3bf-fa924e97b8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/15\n",
      "20000/20000 [==============================] - 8s 389us/step - loss: 0.5877 - acc: 0.7548 - val_loss: 0.5701 - val_acc: 0.7618\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76180, saving model to weigths.best.hdf5\n",
      "Epoch 2/15\n",
      "20000/20000 [==============================] - 8s 377us/step - loss: 0.5128 - acc: 0.7808 - val_loss: 0.4634 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76180 to 0.79700, saving model to weigths.best.hdf5\n",
      "Epoch 3/15\n",
      "20000/20000 [==============================] - 8s 378us/step - loss: 0.3989 - acc: 0.8289 - val_loss: 0.3854 - val_acc: 0.8426\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.79700 to 0.84260, saving model to weigths.best.hdf5\n",
      "Epoch 4/15\n",
      "20000/20000 [==============================] - 8s 380us/step - loss: 0.3412 - acc: 0.8653 - val_loss: 0.3574 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.84260 to 0.85120, saving model to weigths.best.hdf5\n",
      "Epoch 5/15\n",
      "20000/20000 [==============================] - 8s 379us/step - loss: 0.2944 - acc: 0.8883 - val_loss: 0.3396 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85120 to 0.85420, saving model to weigths.best.hdf5\n",
      "Epoch 6/15\n",
      "20000/20000 [==============================] - 8s 378us/step - loss: 0.2726 - acc: 0.8968 - val_loss: 0.3483 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85420 to 0.85720, saving model to weigths.best.hdf5\n",
      "Epoch 7/15\n",
      "20000/20000 [==============================] - 8s 377us/step - loss: 0.2447 - acc: 0.9084 - val_loss: 0.3232 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.85720 to 0.86140, saving model to weigths.best.hdf5\n",
      "Epoch 8/15\n",
      "20000/20000 [==============================] - 8s 379us/step - loss: 0.2220 - acc: 0.9189 - val_loss: 0.3527 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86140\n",
      "Epoch 9/15\n",
      "20000/20000 [==============================] - 8s 382us/step - loss: 0.2029 - acc: 0.9279 - val_loss: 0.3465 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86140\n",
      "Epoch 10/15\n",
      "20000/20000 [==============================] - 8s 378us/step - loss: 0.1883 - acc: 0.9345 - val_loss: 0.3416 - val_acc: 0.8602\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86140\n",
      "Epoch 11/15\n",
      "20000/20000 [==============================] - 8s 379us/step - loss: 0.1766 - acc: 0.9401 - val_loss: 0.3676 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86140\n",
      "Epoch 12/15\n",
      "20000/20000 [==============================] - 8s 379us/step - loss: 0.1674 - acc: 0.9437 - val_loss: 0.3917 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86140\n",
      "Epoch 13/15\n",
      "20000/20000 [==============================] - 8s 382us/step - loss: 0.1553 - acc: 0.9499 - val_loss: 0.3977 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86140\n",
      "Epoch 14/15\n",
      "20000/20000 [==============================] - 8s 380us/step - loss: 0.1469 - acc: 0.9534 - val_loss: 0.4477 - val_acc: 0.8440\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86140\n",
      "Epoch 15/15\n",
      "20000/20000 [==============================] - 7s 373us/step - loss: 0.1398 - acc: 0.9564 - val_loss: 0.4187 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.86140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f777e9530b8>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train, validation_data = [X_valid,Y_valid], epochs = 15, batch_size = 2500, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "2PLVWL6wMHZ6",
    "outputId": "0c1ee4b4-087c-4095-ec76-07c710454c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 83.636\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test,Y_test,verbose=0)\n",
    "print(model.metrics_names[1], scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjdIvAd6flZZ"
   },
   "outputs": [],
   "source": [
    "All 50D\n",
    "\n",
    "#Skipgram-Word2vec - NonTrainable Parameters - Accuracy 85.124\n",
    "#Skipgram-Word2vec - Trainable Parameters - Accuracy 83.52\n",
    "#CBAG-Word2vec - NonTrainable Parameters - Accuracy 85.488\n",
    "#CBAG-Word2vec - Trainable Parameters - Accuracy 85.916\n",
    "#default kears embedding - 82.888\n",
    "#Glove pretrained - Accuracy 83.248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k3FjQRDWGyl-",
    "outputId": "cd3d3803-28e5-4166-ef8e-d711011e4c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  sample_data  weigths.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "os.mknod(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkdJMqa4zOzn"
   },
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\",'w') as json_file:\n",
    "  json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_gCkz0zSIgD"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_NCreAidSOfO"
   },
   "outputs": [],
   "source": [
    "# load weights into new model\n",
    "loaded_model.load_weights(\"weigths.best.hdf5\")\n",
    "print(\"Loaded weigths from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyEeIwZnSQrk"
   },
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_with_Word_Embeddings_keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
